\chapter{Related Work}
\label{ch:relatedwork}

Static Analysis plays a prominent role in releasing bug-free Software. In spite of that, important tools suffer from well-documented usability issues \cite{CB16,JSMB13}.  Maria et. al. \cite{CB16} did an empirical study on what developers want and need from Program Analysis. They noticed that there are some obstacles which hinder the usage of a Static Analysis tool by a developer such as 'Wrong checks are on by default', 'Too many false positives', 'Too slow',  'Complex user interface' etc. Being a user interface an obstacle for a developer is noteworthy. Johnson et. al. \cite{JSMB13} also found design flaws in current Static Analysis tools and the need for an interactive mechanism in assisting developers in fixing bugs. The interesting findings are like if the output of static analysis tool is user friendly and intuitive then false positives and high number of warnings could be less problematic for a developer, showing call hierarchies with which parts of code are affected by a bug, be able to share settings with predefined coding standards among the team, need of a web browser for reacting on the analysis output for instance adding comment to a bug which goes out of context to the developer. The key takes away from the above-mentioned papers is the importance of Usability in the ongoing adaption of Static Analysis tools. \\ \\ 

In general, the setup of most of the recent research \cite{CB16} \cite{JSMB13} done in the area of Static Code Analysis is like assuming a single project in an organisation. Further, they assume there is a single person working on a single project with a single tool tackling a single type of problems.  Somehow, the assumptions are made so singular to address a specific issue in their research. However, in practice i.e., in the real world of software engineering, there are numerous people working in teams for multiple projects at a time. Each project uses multiple tools in their software development. Even in the case of Static Code Analysis, multiple tools are used which are each capable of addressing several types of issues. \\ \\

Caitlin et. al. and team developed a framework called Tricorder \cite{tricorder} which mentions about using multiple tools where each tool covers seperate bug coverage. The results are displayed during Code review and published by a bot called ReviewBot. The evaluation is made as summative apporach using click rates by the user which shows which tool is better in comparision to other. Cristina et. al. and team developed a framework called Parfait \cite{parfait} using multiple tools which address the issues with scalability and precision. For scalability, it checks the easy and expensive analysis for each bug type and for precision it tracks each bug whether it is real, potential or not.
